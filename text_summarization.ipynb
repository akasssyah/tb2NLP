{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI8kgIEV9Jae",
        "outputId": "613d74dc-28b2-48c3-d3c9-99b677666521"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.17.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JdZBpWF9CXM",
        "outputId": "81c0d285-35ad-4a48-c3e6-3be1fbcf0439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Original Text: \n",
            "\n",
            "Technology has revolutionized the way humans interact, work, and live, transforming societies and industries at an unprecedented pace. At its core, technology encompasses the tools, techniques, and systems developed to solve problems, enhance productivity, and improve quality of life. From the invention of the wheel to the age of artificial intelligence, each technological leap has reshaped humanityâ€™s trajectory. In modern times, digital technologies like the internet, smartphones, and cloud computing have interconnected the world, enabling seamless communication and access to information. Advances in fields such as biotechnology, renewable energy, and robotics are addressing global challenges like climate change, healthcare, and resource scarcity. Artificial intelligence and machine learning are automating tasks, driving innovation, and fostering breakthroughs across sectors, from autonomous vehicles to personalized medicine. However, technology also brings challenges, including ethical concerns, data privacy issues, and the digital divide that separates those with access to technology from those without. The rapid pace of technological change requires societies to adapt, balancing innovation with regulation to ensure equitable and sustainable development. As technology continues to evolve, it holds immense potential to shape a future marked by unprecedented possibilities, provided its development aligns with human values and societal needs.\n",
            "\n",
            "Summary: \n",
            "\n",
            "At its core, technology encompasses the tools, techniques, and systems developed to solve problems, enhance productivity, and improve quality of life. However, technology also brings challenges, including ethical concerns, data privacy issues, and the digital divide that separates those with access to technology from those without. Technology has revolutionized the way humans interact, work, and live, transforming societies and industries at an unprecedented pace.\n",
            "\n",
            "Evaluation:\n",
            "\n",
            "ROUGE Scores: [{'rouge-1': {'r': 0.3673469387755102, 'p': 1.0, 'f': 0.5373134289062153}, 'rouge-2': {'r': 0.3160621761658031, 'p': 0.9682539682539683, 'f': 0.4765624962893678}, 'rouge-l': {'r': 0.3673469387755102, 'p': 1.0, 'f': 0.5373134289062153}}]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge import Rouge\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import gensim.downloader as api\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "word_embeddings = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "def read_article(file_name):\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
        "        filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop()\n",
        "    return sentences, filedata[0]\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "def tfidf_similarity(sentences):\n",
        "    # Flatten sentences for TF-IDF\n",
        "    sentences_flat = [\" \".join(sentence) for sentence in sentences]\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences_flat)\n",
        "    return cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "def embedding_similarity(sentences):\n",
        "    def get_sentence_vector(sentence):\n",
        "        vectors = [word_embeddings[w] for w in sentence if w in word_embeddings]\n",
        "        if len(vectors) == 0:\n",
        "            return np.zeros(50)\n",
        "        return np.mean(vectors, axis=0)\n",
        "\n",
        "    sentence_vectors = [get_sentence_vector(sentence) for sentence in sentences]\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(len(sentences)):\n",
        "            if i != j:\n",
        "                similarity_matrix[i][j] = cosine_similarity([sentence_vectors[i]], [sentence_vectors[j]])[0, 0]\n",
        "    return similarity_matrix\n",
        "\n",
        "def gen_sim_matrix(sentences, stop_words, method=\"bag_of_words\"):\n",
        "    if method == \"tfidf\":\n",
        "        return tfidf_similarity(sentences)\n",
        "    elif method == \"embedding\":\n",
        "        return embedding_similarity(sentences)\n",
        "    else:\n",
        "        similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "        for idx1 in range(len(sentences)):\n",
        "            for idx2 in range(len(sentences)):\n",
        "                if idx1 == idx2:\n",
        "                    continue\n",
        "                similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "        return similarity_matrix\n",
        "\n",
        "def evaluate_summary(original_text, summary_text):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary_text, original_text)\n",
        "    return scores\n",
        "\n",
        "def generate_summary(file_name, top_n=5, method=\"bag_of_words\"):\n",
        "    summarize_text = []\n",
        "    sentences, original_text = read_article(file_name)\n",
        "\n",
        "    print(\"Original Text: \\n\")\n",
        "    print(original_text)\n",
        "    print(\"\\nSummary: \\n\")\n",
        "\n",
        "    sentence_similarity_matrix = gen_sim_matrix(sentences, stop_words, method)\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "    ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(\" \".join(ranked_sentence[i][1]) + \".\")\n",
        "\n",
        "    summary_text = \" \".join(summarize_text)\n",
        "    print(summary_text)\n",
        "\n",
        "    rouge_scores = evaluate_summary(original_text, summary_text)\n",
        "    print(\"\\nEvaluation:\\n\")\n",
        "    print(f\"ROUGE Scores: {rouge_scores}\")\n",
        "\n",
        "generate_summary(\"tech.txt\", top_n=3, method=\"embedding\")"
      ]
    }
  ]
}